{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dc461c-bef9-44dd-aa96-28d54ffa07ba",
   "metadata": {},
   "source": [
    "# Amazon Web Scraper\n",
    "\n",
    "In this project, I create a web scraper to extract prices, ratings and other data from Amazon products. It works for the first searching page of the introduced product, but in the future I plan to make it work for several pages of the searched item.\n",
    "\n",
    "The algoritm returns a dataframe of the main search webpage of a product with its price, ratings, number of reviews, date of extraction of the data and a link. And to make it done we use several libraries such as Pandas, BeautifulSoup, Requests and time. Also, the dataframe is converted into documents csv and xlsx for later use.\n",
    "\n",
    "In the practice of this task we face several obstacles that Amazon put in the way to avoid scraping. We need to use the User-Agent of our PC and we need to wait a certain time between each calling to the web to avoid being refused by the server. This parameters might change depending on the location of the user and on the item searched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4666a142-d804-40c5-93fb-619701db7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries needed for the task\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f38165c-60eb-47e0-995d-1493657393ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define each of the functions we will use in the algoritm\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", {\"id\":'productTitle'}).text.strip()\n",
    "    except AttributeError:\n",
    "        title = ''\n",
    "    return title\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = float(soup.find(\"span\", {\"class\":'a-price-whole'}).text.strip().replace(',', '')) + (float(soup.find(\"span\", {\"class\":'a-price-fraction'}).text.strip())/100)\n",
    "    except AttributeError:\n",
    "        price = ''\n",
    "    return price\n",
    "\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"span\", {\"class\":'a-icon-alt'}).text.strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", {\"class\":'a-size-base a-color-base'}).text.strip()\n",
    "        except:\n",
    "            rating = ''\n",
    "    return rating\n",
    "\n",
    "def get_reviews(soup):\n",
    "    try:\n",
    "        reviews = soup.find(\"span\", {\"id\":'acrCustomerReviewText'}).text.strip()\n",
    "    except AttributeError:\n",
    "        reviews = ''\n",
    "    return reviews\n",
    "\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\t\n",
    "\n",
    "    return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a7d1ea-74b8-4bdb-abd3-72e35a353ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next is the main fuction to extract and access the data\n",
    "\n",
    "def amazon_scrapper(product_name):\n",
    "\n",
    "    #We use the product name provided by the user to complete and get the right URL from Amazon\n",
    "    searchterm = product_name.replace(' ', '+')\n",
    "    URL = f'https://www.amazon.com/s?k={searchterm}&__mk_es_US=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=1G0HPO5K3XQPW&sprefix={searchterm}%2Caps%2C151&ref=nb_sb_noss_1'\n",
    "    \n",
    "    #We need to use the headers data from our computer to access the webpage. You can find this on internet\n",
    "    headers = {\"User-Agent\": \"\", \n",
    "                     \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "                     \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\", \n",
    "                     \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    #We use requests and BeautifulSoup to access and parse the webpage\n",
    "    page = requests.get(URL, headers=headers)\n",
    "    soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    #Create a kind of list of each product link with BeautifulSoup\n",
    "    links = soup1.find_all('a', {'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "\n",
    "    #Now we get the actual links of the previous 'list' and put them in an actual list of links\n",
    "    link_list = []\n",
    "    for link in links:\n",
    "        link_list.append('https://www.amazon.com' + link.get('href'))\n",
    "\n",
    "    #Create an empty dictionary to save the data we are going to extract\n",
    "    product_dic = {'Title': [], 'Price': [], 'Rating': [], 'Reviews': [], 'Availability': [], 'Date': [], 'link': []} \n",
    "    \n",
    "    #A loop to extract the data of each product and save it in the dictionary\n",
    "    for link in link_list:\n",
    "        #With this other loop we try to avoid being refused by the server\n",
    "        product_page = ''\n",
    "        while product_page == '':\n",
    "            try:\n",
    "                product_page = requests.get(link, headers=headers)\n",
    "                break\n",
    "            #In case of being refused we put the scraper to sleep a certain time, you can change it if it not works in your case\n",
    "            except:\n",
    "                print(\"Connection refused by the server...\")\n",
    "                print(\"Let me sleep for 120 seconds\")\n",
    "                print(\"Zzzzzz...\")\n",
    "                time.sleep(120)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "\n",
    "        #We parse and extract the data of each product webpage\n",
    "        product_soup = BeautifulSoup(product_page.content, \"html.parser\")\n",
    "        \n",
    "        product_dic['Title'].append(get_title(product_soup))\n",
    "        product_dic['Price'].append(get_price(product_soup))\n",
    "        product_dic['Rating'].append(get_rating(product_soup))\n",
    "        product_dic['Reviews'].append(get_reviews(product_soup))\n",
    "        product_dic['Availability'].append(get_availability(product_soup))\n",
    "        #We add the date and link of each product too\n",
    "        product_dic['Date'].append(datetime.date.today())\n",
    "        product_dic['link'].append(link)\n",
    "        #We use time and sleep library to avoid refusing\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Finaly we convert the dictionary into a dataframe using Pandas, and later into a csv and excel files to save it\n",
    "    amazon_df = pd.DataFrame.from_dict(product_dic)\n",
    "    amazon_df.to_csv(f'{searchterm}.csv', index = False, header = True)\n",
    "    amazon_df.to_excel(f'{searchterm}.xlsx', index = False, header = True)\n",
    "\n",
    "    return amazon_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f581b5c-0fc5-4e0f-8590-e2658bad0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With this we execute the code and visualize the dataframe\n",
    "\n",
    "product_name = input('Enter the product name: ')\n",
    "\n",
    "amazon_df = amazon_scrapper(product_name)\n",
    "\n",
    "amazon_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
